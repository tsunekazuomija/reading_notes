2024-08-03

ResNet [link cvpr2016 open access](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)

OpenPoseの解説記事で言及されてたので読んでみる。
- [参考](https://dx-consultant-fast-evolving.com/resnet/)

google scholarの被引用数が22万件以上あって驚いた。

## Title
Deep Residual Learning for Image Recognition

## Author
Kaiming He Xiangyu Zhang Shaoqing Ren Jian Sun
Microsoft Research

## Motivation
層を深くすると、深くしない場合に比べ少なくとも
精度が落ちることはないと考えられるが、実際は落ちる。

## Method
フィードフォワードなネットワークを追加することで、モデルに出力ではなく
残差を学習させる。
理想的なモデルが、zero mappingよりidentity mappingに近い場合、
斬さを学習させる手法の方が効果的である。

- 18層、34層のプレーンなネットワークと、同じ層数のResNetを比較。
- ボトルネックアーキテクチャを導入して、計算効率を向上させ他モデルを、他のstate of the artなモデルと比較
- CIFAR-10を用いて、モデルの深さを変えた時の振る舞いを細かく調べる。
  - 1000層以上の場合も調査した。学習エラーは極めて小さいが、テストエラーは110層の場合より大きい。overfittingが原因と考えられる。

## Insights



## Cotribution Summary
Heらは、層を深くすることでニューラルネットワークの精度が落ちるという問題に対し、
残差学習を導入することで、収束を早め、精度を向上させることに成功した。
この手法を用いて層を深くしていくことで、一定の深さまでは精度を向上させることができることを示した。

## keyword
- Batch Normalization(BN)
- Bottleneck Architectures
  - 計算効率が上がるらしい
  - [参考](https://qiita.com/koshian2/items/031b6a335d0d217e4c4c)


## Unknown
Experimentにおいてプレーンな34層のネットワークが18層のネットワークより学習誤差が大きいことがわかったが、勾配消失は原因ではないと考えられる。筆者は深いプレーンネットワークが浅いものに比べて指数的に収束しにくいためと考えているが、確証はない。

1000層の場合に、特に正則化をしていない。何かしら工夫をすることで、1000層のような極めて深いネットワークで高い精度を出すことができるかもしれない。

## Reflection
Analysis of Layer Responsesでレイヤーのリスポンスを調べる表を載せているが、大きさ順にソートして見せている。
こういう見せ方があるのか

## Reference
- Faster R-CNN
  - Object Detectionの箇所で言及されている。


# メモ
- FLOPs: Floating Point Operations
  - 計算量のこと。
  - コンピュータアーキテクチャのFLOPS(Floating Point Operations Per Second)とは異なる。
  - [参考](https://qiita.com/Haaamaaaaa/items/4277a472f4689d72de20)

- Implementationはわからん。実際にハンズオンで勉強しないと理解できるようにはならないかも

- time complexity

- アンサンブル学習