2024-08-06

## Title
Attention is All You Need

- [link]
- [github](https://github.com/tensorflow/tensor2tensor)

## Author
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I.

Google Brain, Google Research

## Motivation
リカレントモデルは、前の隠れ状態を次の隠れ状態を計算する際の入力として使用するため、並列化が難しい。

アテンション機構は、入力や出力の中での距離に関係なく依存関係をモデル化することができ、説得力のあるsequence modelingにふかけつである。しかし、ほとんどの場合リカレントモデルと組み合わせて使用される。

<!-- 入出力のグローバルな依存関係を引き出すのに -->

## Method
RNN, CNNを使用していない。エンコーダ・デコーダモデル

### エンコーダ
- Multi-Head self-attention 機構とpoint-wise feed-forward networksからなるレイヤーを6つ重ねる。残差ネットワークと正規化を使用。

### デコーダ
- エンコーダと同様に6つのレイヤーからなるが、サブレイヤーにエンコーダの出力をmulti-head self-attention を行うレイヤーを追加。
- 自分より前の位置のみを参照するように修正したattention機構を使用。

## Insights
recurent, convolutionalモデルと比較して学習が早い。
WMT 2014 English-to-German and WMT 2014　English-to-French translation tasks　においてsotaを達成. English-to-Germanでは過去の最良のアンサンブルモデルを上回った。

## Cotribution Summary


## keyword
- multi-head self-attention

## Unknown
- 他のタスクへのTransformerの適用
- テキスト以外のモダリティへの適用
- 極めて大きな入力へ適用するための、ローカル情報へのアテンションの制約
- 生成の際の直列性を減らす

## Reflection


## Reference


## Note

Human Pose Estimationの総説論文でTransformerを使用した研究が紹介されていたので、Transformerについて調べてみる。

そもそもAttention機構や機械翻訳についての知識がないので、適宜ググる。

参考
- [キカガクのTransformerの解説](https://www.kikagaku.co.jp/kikagaku-blog/deep-learning-transformer/)
  - エンコーダデコーダモデルについて
- [Attension機構についてのQiitaの記事](https://qiita.com/ps010/items/0bb2931b666fa602d0fc)

- Transformerはそれ以前のエンコーダ・デコーダモデルに比較して、何が画期的なのか？

Methodの理解が難しいので、もう少し日本語のソースを漁ったり、
ハンズオンで実装してみた後に再度読む。


