2024-08-07

## Title
Motion Guided 3D Pose Estimation from Videos

## Author
Jingbo Wang1 , Sijie Yan1 , Yuanjun Xiong2 , and Dahua Lin1

1The Chinese University of Hong Kong 2AWS / Amazon AI, Hong Kong

## Motivation
動画からの3DHPEで一般的に損失関数としてMinkovski距離が使用されているが、これは時系列の構造を考慮できていない。時系列の構造を考慮することで、

## Method
motion lossを導入。
モデルにキーポイントの3d座標及びその軌道を予測させる。
motion encodingという表現空間を導入し、その中でmotion lossにより予測を評価。
motion encodingは、2つの時点のキーポイントの位置座標に対し、微分可能な二項演算をてきようして、得られた結果を連結することで得られる。ただし、2時刻のインターバルは様々であり、これが様々なタイムスケールを表現している。

motion encodingの2項演算として、差、内積、クロス積を用いて性能を比較


STGCNという骨格ベースのアクション認識モデルをベースにしており、またU-Shaped CNNを参考にして、あらたにU-shaped GCNを提案。
短期、長期的な時間的依存関係を考慮できるようになった。

Human3.6MとMPI-INF-3DHPデータセットで実験を行い、性能を評価
ablation studyを行い、モデルのポテンシャルについてさらに議論する

2ステージ(RGB -> 2D pose -> 3d pose)のアプローチを採用.データの次元を落とすことができるため、長時間のビデオベース3DHPEに適している。



## Insights
motion lossの導入により、位置の精度が向上したことを確認。

UGCNと組み合わせることで、sotaを達成した。また、smoothiness constraintを導入することなく、スムーズなモーションを再現できるようになった。
また、他のsotaに比べ、velocity errorを半減させた。


## Cotribution Summary


## keyword
- U-shaped CNN
- st-graph
- graph convolution

## Unknown


## Reflection
motion lossの重要性も、UGCNの有用性もなんとなく理解できたが、これらを1つの論文にまとめた理由がよくわからない。motion lossを導入して初めてUGCNを提案できるということ？

## Reference
- STGCN
  - Yan, S., Xiong, Y., Lin, D.: Spatial temporal graph convolutional networks for skeleton-based action recognition. In: Thirty-Second AAAI Conference on Artificial Intelligence (2018)

## Note
graph convolutionの部分さえわかれば理解できそう。